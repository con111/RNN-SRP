```{r}
#Step 1 install packages
#install necessary packages
install.packages("ggplot2")
install.packages("dplyr")

#Step 2 load the libraries
# Load required packages
library(ggplot2)
library(dplyr)

#Step 3 Load the dataset
data <- read.csv("C:/Users/conno/OneDrive/Documents/SRP_summer_research/planar_flower.csv")
data

#Step 4 Preprocessing
X <- data %>% select(x1, x2) %>% as.matrix()
Y <- as.matrix(data$y)

# Visualize the data
df <- data.frame(x = X[,1], y = X[,2], label = as.factor(Y))
ggplot(df, aes(x=x, y=y, color=label)) + geom_point() + theme_minimal()

# Normalize features
X <- scale(X)

# Step 5 Parameter Initalization
# Split data into training and testing sets
set.seed(69)
train_indices <- sample(1:nrow(X), size = 0.8 * nrow(X))
X_train <- X[train_indices, ]
Y_train <- Y[train_indices]
X_test <- X[-train_indices, ]
Y_test <- Y[-train_indices, ]

# Initalize RNN parameters
input_size <- 2  # number of ndes in the input layer (in this case the nodes represent x_1 and x_2 )
hidden_size <- 4  # number of nodes in the hidden layer
output_size <- 1  # number of neurons in the output
learning_rate <- 0.01
num_epochs <- 1000

# Step 6 Activation Functions
sigmoid <- function(x) {
  return(1 / (1 + exp(-x)))
}

sigmoid_derivative <- function(x) {
  return(x * (1 - x))
}

# Step 7 Initialize Weights and Biases
W_xh <- matrix(runif(input_size * hidden_size, -0.5, 0.5), nrow = input_size)
W_hh <- matrix(runif(hidden_size * hidden_size, -0.5, 0.5), nrow = hidden_size)
W_hy <- matrix(runif(hidden_size * output_size, -0.5, 0.5), nrow = hidden_size)
b_h <- matrix(0, nrow = 1, ncol = hidden_size)
b_y <- matrix(0, nrow = 1, ncol = output_size)

#Step 8 Forward Propgation
forward <- function(inputs, hidden) {
  outputs <- list()
  for (t in 1:nrow(inputs)) {
    hidden <- sigmoid(inputs[t, , drop = FALSE] %*% W_xh + hidden %*% W_hh + b_h)
    output <- sigmoid(hidden %*% W_hy + b_y)
    outputs[[t]] <- output
  }
  return(list(outputs = outputs, hidden = hidden))
}

# Step 9: Training Function with Regularization
train <- function(inputs, targets) {
  hidden <- matrix(0, nrow = 1, ncol = hidden_size)
  for (t in 1:nrow(inputs)) {
    forward_result <- forward(matrix(inputs[t, ], nrow = 1), hidden)
    output <- forward_result$outputs[[1]]
    hidden <- forward_result$hidden
    
    output_error <- output - targets[t]
    hidden_error <- output_error %*% t(W_hy) * sigmoid_derivative(hidden)
    
    dW_xh <- t(matrix(inputs[t, ], nrow = 1)) %*% hidden_error
    dW_hh <- t(hidden) %*% hidden_error
    dW_hy <- t(hidden) %*% output_error
    db_h <- hidden_error
    db_y <- output_error
    
    # Update weights and biases with regularization
    W_xh <<- W_xh - learning_rate * (dW_xh + lambda * W_xh)
    W_hh <<- W_hh - learning_rate * (dW_hh + lambda * W_hh)
    W_hy <<- W_hy - learning_rate * (dW_hy + lambda * W_hy)
    b_h <<- b_h - learning_rate * db_h
    b_y <<- b_y - learning_rate * db_y
  }
}

# Step 10: Helper Functions for Evaluation Metrics
accuracy <- function(predictions, targets) {
  return(mean((predictions > 0.5) == targets))
}

precision <- function(predictions, targets) {
  tp <- sum((predictions > 0.5) & (targets == 1))
  fp <- sum((predictions > 0.5) & (targets == 0))
  return(tp / (tp + fp))
}

recall <- function(predictions, targets) {
  tp <- sum((predictions > 0.5) & (targets == 1))
  fn <- sum((predictions <= 0.5) & (targets == 1))
  return(tp / (tp + fn))
}

f1_score <- function(precision, recall) {
  return(2 * precision * recall / (precision + recall))
}

# Step 11: Training Loop with Early Stopping
best_loss <- Inf
patience <- 50
patience_counter <- 0

for (epoch in 1:num_epochs) {
  loss <- 0
  for (i in 1:nrow(X_train)) {
    inputs <- matrix(X_train[i, ], nrow = 1)
    targets <- matrix(Y_train[i], nrow = 1)
    
    train(inputs, targets)
    forward_result <- forward(inputs, matrix(0, nrow = 1, ncol = hidden_size))
    output <- forward_result$outputs[[1]]
    loss <- loss + sum((output - targets) ^ 2)
  }
  
  loss <- loss / nrow(X_train)
  
  if (loss < best_loss) {
    best_loss <- loss
    patience_counter <- 0
  } else {
    patience_counter <- patience_counter + 1
  }
  
  if (patience_counter > patience) {
    cat("Early stopping at epoch", epoch, "with best loss", best_loss, "\n")
    break
  }
  
  pred_train <- sapply(1:nrow(X_train), function(i) {
    inputs <- matrix(X_train[i, ], nrow = 1)
    forward_result <- forward(inputs, matrix(0, nrow = 1, ncol = hidden_size))
    return(forward_result$outputs[[1]])
  })
  
  acc <- accuracy(pred_train, Y_train)
  prec <- precision(pred_train, Y_train)
  rec <- recall(pred_train, Y_train)
  f1 <- f1_score(prec, rec)
  
  cat("Epoch:", epoch, "Loss:", loss, "Accuracy:", acc, "Precision:", prec, "Recall:", rec, "F1 Score:", f1, "\n")
}

#Step 12 Test RNN
pred_test <- sapply(1:nrow(X_test), function(i) {
  inputs <- matrix(X_test[i, ], nrow = 1)
  forward_result <- forward(inputs, matrix(0, nrow = 1, ncol = hidden_size))
  return(forward_result$outputs[[1]])
})

acc_test <- accuracy(pred_test, Y_test)
prec_test <- precision(pred_test, Y_test)
rec_test <- recall(pred_test, Y_test)
f1_test <- f1_score(prec_test, rec_test)

cat("Test Accuracy:", acc_test, "Test Precision:", prec_test, "Test Recall:", rec_test, "Test F1 Score:", f1_test, "\n")
```







